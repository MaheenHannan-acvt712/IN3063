{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec573de",
   "metadata": {},
   "source": [
    "Importing NumPy and DatasetGenerator class which contains custom function for handling MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6956dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset_generator import DatasetGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58eb0f4",
   "metadata": {},
   "source": [
    "ReLU activation and gradient function; the activation function gets an array and sets the data inside it to non-negative values. The gradient function converts a given array into a boolean based array where each value above 0 is set to 1 (True) or 0 (False) if lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e980af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relu(func, activation=False, gradient=False):\n",
    "    if activation and not gradient:\n",
    "        relu = np.maximum(0, func)\n",
    "        return relu\n",
    "    elif not activation and gradient:\n",
    "        gradient = np.where(func > 0, 1, 0)\n",
    "        return gradient\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f94de7",
   "metadata": {},
   "source": [
    "Sigmoid activation and gradient function; the activation function gets an array and dervives each element in it with the mathimatical function 1 divided by the sum of 1 divided by the negative value of each array element. The gradient function reuses the sigmoid activation function but supplements it with a multiplication of derivative Sigmoid's gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43354e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigmoid(func, activation=False, gradient=False):\n",
    "    sigmoid = 1 / (1 + np.exp(-func))\n",
    "    if activation and not gradient:\n",
    "        return sigmoid\n",
    "    elif not activation and gradient:\n",
    "        gradient = sigmoid * (1 - sigmoid)\n",
    "        return gradient\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db77b13f",
   "metadata": {},
   "source": [
    "Only the Softmax gradient; have had trouble implementing the activation code. The gradient assumes the model of dataset labels and gets the sample size of MNIST as the batch size, it then calculates the gradient by calculating the difference between the model (prediction) and the real answers (truth labels) which is then divided by the batch size obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4cfaece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_softmax(func=None, model_labels=None, truth_labels=None, activation=False, gradient=False):\n",
    "    batch_size = model_labels.shape[0]\n",
    "    gradient = (model_labels - truth_labels) / batch_size\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00bd95",
   "metadata": {},
   "source": [
    "Initialisation of the neural network with hyperparameters and help from the DatasetGenerator class to calculate the input and output layers, total layer architecture is then built using user choice of first and second hidden layers.\n",
    "\n",
    "The biases calculation is done by first creating an range of non-negative numbers which are then used to create an array where each element is neuron connected to the next layer.\n",
    "\n",
    "The weights are calculated creating a maxtrix of random integers based on the layers given; each value is then scaled to the squareroot of half the value of the current layer in the interation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8796f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, hidden_layers, activation):\n",
    "        input_layers, output_layers = DatasetGenerator().get_layers()\n",
    "        hidden_layers.insert(0, input_layers)\n",
    "        hidden_layers.append(output_layers)\n",
    "\n",
    "        self.layers = hidden_layers\n",
    "        self.num_layers = len(self.layers)\n",
    "        self.biases = [np.zeros((1, self.layers[i + 1])) for i in range(self.num_layers - 1)]\n",
    "        self.activation = activation\n",
    "        self.activations = []\n",
    "        self.vect_transfer_list = []\n",
    "\n",
    "        self.weights = []\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            random_matrix = np.random.randn(self.layers[layer], self.layers[layer + 1])\n",
    "            scale = np.sqrt(2 / self.layers[layer])\n",
    "            self.weights.append(random_matrix * scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1d2c2",
   "metadata": {},
   "source": [
    "Forward pass returning the output which is the dot product of the weight of the current layer added with the value of the bias value of the same layer. This vector transformation is added to a list of transformations - then depending on the choice of activation (sigmoid or relu) the output is \"activated\" and added into the list of activated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37182031",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, data):\n",
    "        self.activations = []\n",
    "        self.vect_transfer_list = []\n",
    "\n",
    "        outputs = data\n",
    "        self.activations.append(outputs)\n",
    "\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            i = np.dot(outputs, self.weights[layer])\n",
    "            vect_transfer = i + self.biases[layer]\n",
    "            self.vect_transfer_list.append(vect_transfer)\n",
    "\n",
    "            if self.activation == 'sigmoid':\n",
    "                outputs = get_sigmoid(vect_transfer, activation=True)\n",
    "            elif self.activation == 'relu':\n",
    "                outputs = get_relu(vect_transfer, activation=True)\n",
    "\n",
    "            self.activations.append(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b850e",
   "metadata": {},
   "source": [
    "Backward pass calculates the gradients from the output layer back to the input layer (reverse order of layers). The gradients take in the delta sum of the current and next layers then adjusting the weights and biases of the traversed layers to reduce loss for when the epoch is set initialised. The delta, error rate, between current and previous layers are calculated used softmax gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586231ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, data, truth_labels, model_labels, learning_rate):\n",
    "        batch_size = data.shape[0]\n",
    "        gradients_weights = []\n",
    "        gradients_biases = []\n",
    "\n",
    "        if self.activation == 'sigmoid':\n",
    "            activation_gradient = get_sigmoid\n",
    "        elif self.activation == 'relu':\n",
    "            activation_gradient = get_relu\n",
    "\n",
    "        delta = get_softmax(model_labels=model_labels, truth_labels=truth_labels, gradient=True)\n",
    "\n",
    "        for layer in range(self.num_layers - 1, 0, -1):\n",
    "            output = self.activations[layer - 1]\n",
    "            vect_transfer = self.vect_transfer_list[layer - 1]\n",
    "\n",
    "            gradients_vect_transform = activation_gradient(vect_transfer, gradient=True)\n",
    "            gradients_vect_transform = gradients_vect_transform * delta\n",
    "\n",
    "            gradients_weight = np.dot(output.T, gradients_vect_transform)\n",
    "            gradients_weight = gradients_weight / batch_size\n",
    "\n",
    "            gradients_bias = np.sum(gradients_vect_transform, axis=0, keepdims=True)\n",
    "            gradients_bias = gradients_bias / batch_size\n",
    "\n",
    "            delta = np.dot(gradients_vect_transform, self.weights[layer - 1].T)\n",
    "\n",
    "            gradients_weights.append(gradients_weight)\n",
    "            gradients_biases.append(gradients_bias)\n",
    "\n",
    "        gradients_weights.reverse()\n",
    "        gradients_biases.reverse()\n",
    "\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            self.weights[layer] -= learning_rate * gradients_weights[layer]\n",
    "            self.biases[layer] -= learning_rate * gradients_biases[layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4524b6",
   "metadata": {},
   "source": [
    "The loss calculation and prediction functions are both using the labels from the data subsets to:\n",
    "\n",
    "    Divide the negative sum of labels (truth) multiplied by non-infinity log values of predicited labels (model) divided by the number of samples in the data subset (this is a interpertation of cross-entropy loss)\n",
    "    \n",
    "    Run the predicted answers through the nueral network and identify the answers using the highest score for the given label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d83a82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def calculate_loss(self, data, truth_labels):\n",
    "        model_labels = self.forward(data)\n",
    "        loss = -np.sum(truth_labels * np.log(model_labels + 1e-10)) / data.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def predict(self, data):\n",
    "        model_label = self.forward(data)\n",
    "        return np.argmax(model_label, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
