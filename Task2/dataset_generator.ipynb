{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e8732f",
   "metadata": {},
   "source": [
    "Importing NumPy and TorchVision to get the MNIST dataset and convert them to NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646073b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad844a",
   "metadata": {},
   "source": [
    "Initialistion of the DatasetGenerator class where the number of workers, batch sizes, and valid sizes used to get and build the datasets are defined.\n",
    "Setting the number of workers to 0 causes the data preperation to be handled by the main thread.\n",
    "Batch size and valid size refers to the actual processing of the dataset; batch size of 20 is setting the size of each minibatch and 0.2 is a percentage (20%) of how much the dataset is used for validation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263afb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    def __init__(self):\n",
    "        self.num_workers = 0\n",
    "        self.batch_size = 20\n",
    "        self.valid_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648b556",
   "metadata": {},
   "source": [
    "Using the TorchVision functions to get the MNIST dataset and setting its download locatations. The train valirable is used to set the download to either getting the training or testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f95b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def dataset(self, train=True):\n",
    "        return datasets.MNIST(root='data', train=train, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b422de0",
   "metadata": {},
   "source": [
    "This function converts the MNIST Torch dataset into a NumPy array used later for all the NumPy based calculations.\n",
    "\n",
    "    1. First the X and Y of the datasets are obtained\n",
    "    2. They are both turned into non-singletons and made easier to be operated on\n",
    "    3. The data (images of hand written numbers) is normalised by 255.0 (greyscale bitsize)\n",
    "    4. Flatten the data into a 2D array\n",
    "    5. Conver the labels into a categorical array of arrays where each category is binary (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c00d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def to_numpy(self, dataset):\n",
    "        data = dataset.data.numpy()\n",
    "        labels = dataset.targets.numpy()\n",
    "        data = np.squeeze(np.array(data))\n",
    "        labels = np.squeeze(np.array(labels))\n",
    "        data = data / 255.0\n",
    "        data_flat = data.reshape(data.shape[0], -1)\n",
    "        labels = np.eye(10)[labels]\n",
    "        return data_flat, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d59c5a",
   "metadata": {},
   "source": [
    "Returns a NumPy array converted train dataset of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5809ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_train_data(self):\n",
    "        return self.to_numpy(self.dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace5f1e",
   "metadata": {},
   "source": [
    "Returns a NumPy array converted test dataset of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54126a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_test_data(self):\n",
    "        return self.to_numpy(self.dataset(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd5e89",
   "metadata": {},
   "source": [
    "Returns the \"shape\" of the dataset used later for hyperparamterisation and layer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5b8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_layers(self):\n",
    "        data = self.dataset().data[0].shape[0]\n",
    "        labels = len(np.unique(self.dataset().targets))\n",
    "        return (data * data), labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
