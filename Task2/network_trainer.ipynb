{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80c8d5e",
   "metadata": {},
   "source": [
    "Importing NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc7ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58c2587",
   "metadata": {},
   "source": [
    "Initialisation of the NetworkTrainer class and passing in the NeuralNetwork that will be defined during execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f243733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkTrainer:\n",
    "    def __init__(self, neural_network):\n",
    "        self.neural_network = neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe961a",
   "metadata": {},
   "source": [
    "Creating a partial SGD optimiser that gets model predictions via forward pass, sends it to a backward pass to calculate gradient and then updates the weights and biases. The other steps of SGD implementation is done during the train_network function where it is applied in mini-batches and iteritively through epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd5bf827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(self, data, truth_labels, learning_rate):\n",
    "        model_labels = self.neural_network.forward(data)\n",
    "        self.neural_network.backward(data, truth_labels, model_labels, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3852876",
   "metadata": {},
   "source": [
    "Defining and adding in the function arguements which take the following:\n",
    "\n",
    "    1. Data from train dataset\n",
    "    2. Labels from the train dataset\n",
    "    3. Data from the test dataset\n",
    "    4. Labels from the test dataset\n",
    "    5. Learning rate which defines the convergance step size during backward pass\n",
    "    6. Epochs which defines how many times the dataset will be trained, tested, and validated\n",
    "    7. Batch size which defines the number of samples to be used\n",
    "    \n",
    "Followed by the iteration of the dataset and creating an array of the data of the train dataset and creating shuffled batches, during each iteration or epoch, another iteration occures wherein batches of the dataset are turned into mini-batches that.\n",
    "\n",
    "Each mini-batch is extrapolated into a sets of data and labels that pass through the neural network's forward pass, then through the SGD single-step optimiser, then finally into a backwards pass. After the mini-batches are trained, a loss is computed to identify how well the neural network is doing.\n",
    "\n",
    "Finally the neural network and model are tested followed by how well each test was - in this case, during each mini-batch iteration, the neural network is tested to give clearer insight.\n",
    "\n",
    "Lastly, once the neural network has gone through it final computation, the results are collected and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae4e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(self,\n",
    "                      data_train, labels_train,\n",
    "                      data_test, labels_test,\n",
    "                      learning_rate=0.1,\n",
    "                      epochs=1,\n",
    "                      batch_size=32):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "            array = data_train.shape[0]\n",
    "            batches = np.arange(array)\n",
    "            np.random.shuffle(batches)\n",
    "            \n",
    "    for batch in range(0, array, batch_size):\n",
    "                eob = batch + batch_size\n",
    "                mini_batch = batches[batch:eob]\n",
    "                data_batch = data_train[mini_batch]\n",
    "                labels_batch = labels_train[mini_batch]\n",
    "                self.sgd(data_batch, labels_batch, learning_rate)\n",
    "\n",
    "                model_labels = self.neural_network.forward(data_batch)\n",
    "                self.neural_network.backward(data_batch, labels_batch, model_labels, learning_rate)\n",
    "\n",
    "                loss = self.neural_network.calculate_loss(data_train, labels_train)\n",
    "\n",
    "                model_label = self.neural_network.predict(data_test)\n",
    "                accuracy = np.mean(model_label == np.argmax(labels_test, axis=1))\n",
    "\n",
    "                epoch_print = f\"Epoch {epoch + 1}/{epochs}\"\n",
    "                step_print = f\"Step: {int(batch / batch_size)}\"\n",
    "                loss_print = f\"Loss: {loss:.4f}\"\n",
    "                test_print = f\"Accuracy: {accuracy:.4f}\"\n",
    "\n",
    "                print(epoch_print, step_print, loss_print, test_print)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
